{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa476b3d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import onnxruntime as rt\n",
    "import onnx\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from skl2onnx import to_onnx\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skl2onnx import convert_sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# define a XGBoost classifier\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore runtime warnings\n",
    "# Temporarily adjust pandas display settings for large DataFrames\n",
    "pd.set_option('display.max_rows', 100)  # Ensure 100 rows can be displayed\n",
    "pd.set_option('display.max_columns', None)  # Ensure all columns can be displayed\n",
    "pd.set_option('display.width', None)  # Automatically adjust display width to terminal size\n",
    "pd.set_option('display.max_colwidth', None)  # Ensure full width of column content is shown\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # Format the float numbers for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c68733094bb7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data preprocessing and feature selection\n",
    "\n",
    "Our data consists of binary data so we only want to calculate the Z-score for non-binary colomns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74ac1f2e39d06",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data/synth_data_for_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e08d21e4405f83b",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "print(\"Before cleaning:\")\n",
    "print(\"Missing values per column:\")\n",
    "print(\"Total missing values:\", data.isna().sum().sum())\n",
    "\n",
    "# Identify non-binary columns\n",
    "non_binary_columns = [col for col in data.columns if not (np.isin(data[col].unique(), [0, 1]).all() and len(data[col].unique()) == 2)]\n",
    "\n",
    "# Calculate Z-scores for non-binary columns only\n",
    "z_scores_non_binary = np.abs(stats.zscore(data[non_binary_columns], nan_policy='omit'))\n",
    "\n",
    "# Mask to identify rows with outliers in non-binary columns\n",
    "outlier_mask = (z_scores_non_binary > 3.5).any(axis=1)\n",
    "\n",
    "# Select a subset of non-binary columns for plotting to avoid large image sizes\n",
    "plot_columns = non_binary_columns[:5]  # Adjust this number based on your specific needs\n",
    "\n",
    "# Plot outliers for the selected columns before removing\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, col in enumerate(plot_columns, 1):\n",
    "    plt.subplot(1, len(plot_columns), i)\n",
    "    sns.boxplot(y=data[col])\n",
    "    plt.title(f'Before: {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove outliers from the dataset using the previously defined full_outlier_mask\n",
    "data_cleaned = data[~outlier_mask]\n",
    "\n",
    "print(\"After cleaning:\")\n",
    "print(\"Missing values per column:\")\n",
    "print(\"Total missing values:\", data_cleaned.isna().sum().sum())\n",
    "\n",
    "# Plot outliers for the selected columns after removing\n",
    "plt.figure(figsize=(20, 5))\n",
    "for i, col in enumerate(plot_columns, 1):\n",
    "    plt.subplot(1, len(plot_columns), i)\n",
    "    sns.boxplot(y=data_cleaned[col])\n",
    "    plt.title(f'After: {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the shape of the dataset before and after cleaning\n",
    "print(\"Shape before cleaning:\", data.shape)\n",
    "print(\"Shape after cleaning:\", data_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80c92f08df57db",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class FeatureZeroing(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, non_fair_keywords):\n",
    "        self.non_fair_keywords = non_fair_keywords\n",
    "        self.removed_features = []  # Initialize list of zeroed features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting process needed for zeroing features, but method required for pipeline compatibility\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert DataFrame to a copy to avoid changing original data\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Make the keywords case-insensitive\n",
    "        non_fair_keywords_lower = [keyword.lower() for keyword in self.non_fair_keywords]\n",
    "\n",
    "        # Identify and zero out non-fair features\n",
    "        for feature in X_transformed.columns:\n",
    "            if any(nfk in feature.lower() for nfk in non_fair_keywords_lower):\n",
    "                X_transformed[feature] = 0\n",
    "                self.removed_features.append(feature)  # Track zeroed features\n",
    "\n",
    "        # Return modified DataFrame\n",
    "        return X_transformed\n",
    "\n",
    "# Define non-fair keywords (same as before)\n",
    "non_fair_keywords = [\n",
    "    \"adres\", \"woonadres\", \"verzendadres\", \"buurt\", \"wijk\", \"plaats\", \"persoon_geslacht_vrouw\", \"taal\", \"kind\", \"ontheffing\", \"leeftijd\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81b41d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Let's specify the features and the target\n",
    "y = data_cleaned['checked']\n",
    "X = data_cleaned.drop(['checked'], axis=1)\n",
    "X = X.astype(np.float32)\n",
    "    \n",
    "# Let's split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)\n",
    "\n",
    "# Initialize the modified feature filter with the non-fair keywords\n",
    "feature_zeroing = FeatureZeroing(non_fair_keywords=non_fair_keywords)\n",
    "\n",
    "# Apply the feature zeroing transformation to the training dataset\n",
    "X_train = feature_zeroing.transform(X_train)\n",
    "\n",
    "# Print out all the features that have been zeroed out\n",
    "print(\"Features zeroed out:\")\n",
    "for feature in feature_zeroing.removed_features:\n",
    "    print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8911074abf9ebe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Feature scaling and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae8fe8386fc9aa",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Initialize SelectFromModel using the classifier to determine feature importances\n",
    "sfm = SelectFromModel(clf, threshold='mean')  # Adjust threshold as needed\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('feature_selection', sfm),\n",
    "    ('classification', classifier)\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeab6096d853145e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Let's evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(f'Precision: {precision:.4f}')\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(f'Recall: {recall:.4f}')\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "\n",
    "# print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(pipeline, X_train, y_train, train_sizes=np.linspace(0.1, 1.0, 10), cv=5, n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a2a6e3cd59268a7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be836516cb7dd1f2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Let's convert the model to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    pipeline, initial_types=[('X', FloatTensorType((None, X.shape[1])))],\n",
    "    target_opset=12)\n",
    "\n",
    "# Let's check the accuracy of the converted model\n",
    "sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "y_pred_onnx =  sess.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f68f63d",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Let's save the model\n",
    "onnx.save(onnx_model, \"model/good_model.onnx\")\n",
    "\n",
    "# Let's load the model\n",
    "new_session = rt.InferenceSession(\"model/good_model.onnx\")\n",
    "\n",
    "# Let's predict the target\n",
    "y_pred_onnx2 =  new_session.run(None, {'X': X_test.values.astype(np.float32)})\n",
    "\n",
    "accuracy_onnx_model = accuracy_score(y_test, y_pred_onnx2[0])\n",
    "print('Accuracy of the ONNX model: ', accuracy_onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05f047",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Check how imbalanced the training and testing datasets are\n",
    "\n",
    "y_train.value_counts(normalize=True)\n",
    "# y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4ba36",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Metamorphic testing: (Other than fairness testing)\n",
    "# If a value changes then the prediction likelihood should change too in line with the purpose of the model \n",
    "# pla_historie_ontwikkeling 0 or 25 // number of developments in PLA history\n",
    "\n",
    "# Load the model\n",
    "testing_session = rt.InferenceSession(\"model/good_model.onnx\")\n",
    "\n",
    "\n",
    "# Initialize variables to store likelihoods\n",
    "likelihoods_0 = []\n",
    "likelihoods_25 = []\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for 0 developments in PLA history\n",
    "    X_sample_0 = X_sample.copy()\n",
    "    X_sample_0['pla_historie_ontwikkeling'] = 0\n",
    "    y_proba_0 = testing_session.run(None, {'X': X_sample_0.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Make predictions for 25 developments in PLA history\n",
    "    X_sample_25 = X_sample.copy()\n",
    "    X_sample_25['pla_historie_ontwikkeling'] = 25\n",
    "    y_proba_25 = testing_session.run(None, {'X': X_sample_25.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Append the likelihoods for both age groups\n",
    "    likelihoods_0.append([y_proba_0[1]])  # Probability of class 1 (fraud) for 0 developments in PLA history\n",
    "    likelihoods_25.append([y_proba_25[1]])  # Probability of class 1 (fraud) for 25 developments in PLA history\n",
    "\n",
    "# Convert likelihoods lists to NumPy arrays\n",
    "likelihoods_0 = np.array(likelihoods_0)\n",
    "likelihoods_25 = np.array(likelihoods_25)\n",
    "\n",
    "# Calculate the mean likelihoods for each group\n",
    "mean_likelihood_0 = np.mean(likelihoods_0)\n",
    "mean_likelihood_25 = np.mean(likelihoods_25)\n",
    "\n",
    "print(\"Mean likelihood for 0 developments in PLA history:\", mean_likelihood_0)\n",
    "print(\"Mean likelihood for 25 developments in PLA history:\", mean_likelihood_25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716dba05",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# contacten_onderwerp_no_show // Contact subject client has not shown up for meeting\n",
    "likelihoods_show = []\n",
    "likelihoods_noshow = []\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for a client that has shown up for meetings\n",
    "    X_sample_show = X_sample.copy()\n",
    "    X_sample_show['contacten_onderwerp_no_show'] = 0.0\n",
    "    y_proba_show = testing_session.run(None, {'X': X_sample_show.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Make predictions for no show client\n",
    "    X_sample_noshow = X_sample.copy()\n",
    "    X_sample_noshow['contacten_onderwerp_no_show'] = 1.0\n",
    "    y_proba_noshow = testing_session.run(None, {'X': X_sample_noshow.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Append the likelihoods for both age groups\n",
    "    likelihoods_show.append([y_proba_show[1]])  # Probability of class 1 (fraud) for a client that has shown up for meetings\n",
    "    likelihoods_noshow.append([y_proba_noshow[1]])  # Probability of class 1 (fraud) for no show client\n",
    "\n",
    "# Convert likelihoods lists to NumPy arrays\n",
    "likelihoods_show = np.array(likelihoods_show)\n",
    "likelihoods_noshow = np.array(likelihoods_noshow)\n",
    "\n",
    "# Calculate the mean likelihoods for each group\n",
    "mean_likelihood_show = np.mean(likelihoods_show)\n",
    "mean_likelihood_noshow = np.mean(likelihoods_noshow)\n",
    "\n",
    "print(\"Mean likelihood for a client that has shown up for meetings:\", mean_likelihood_show)\n",
    "print(\"Mean likelihood for no show client:\", mean_likelihood_noshow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d018b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# instrument_ladder_huidig_activering // instrument ladder is currently activated\n",
    "likelihoods_notactivated = []\n",
    "likelihoods_activated = []\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for a client without an activated instrument ladder\n",
    "    X_sample_notactivated = X_sample.copy()\n",
    "    X_sample_notactivated['instrument_ladder_huidig_activering'] = 0.0\n",
    "    y_proba_notactivated = testing_session.run(None, {'X': X_sample_notactivated.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Make predictions for a client with an activated instrument ladder\n",
    "    X_sample_activated = X_sample.copy()\n",
    "    X_sample_activated['instrument_ladder_huidig_activering'] = 1.0\n",
    "    y_proba_activated = testing_session.run(None, {'X': X_sample_activated.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Append the likelihoods for both age groups\n",
    "    likelihoods_notactivated.append([y_proba_notactivated[1]])  # Probability of class 1 (fraud) for a client without an activated instrument ladder\n",
    "    likelihoods_activated.append([y_proba_activated[1]])  # Probability of class 1 (fraud) for a client with an activated instrument ladder\n",
    "\n",
    "# Convert likelihoods lists to NumPy arrays\n",
    "likelihoods_notactivated = np.array(likelihoods_notactivated)\n",
    "likelihoods_activated = np.array(likelihoods_activated)\n",
    "\n",
    "# Calculate the mean likelihoods for each group\n",
    "mean_likelihood_notactivated = np.mean(likelihoods_notactivated)\n",
    "mean_likelihood_activated = np.mean(likelihoods_activated)\n",
    "\n",
    "print(\"Mean likelihood for a client without an activated instrument ladder:\", mean_likelihood_notactivated)\n",
    "print(\"Mean likelihood for a client with an activated instrument ladder:\", mean_likelihood_activated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011c2b2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# instrument_reden_beeindiging_historie_succesvol // successful instrumentation history\n",
    "likelihoods_not = []\n",
    "likelihoods_successful = []\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for a client without a successful instrumentation history\n",
    "    X_sample_not = X_sample.copy()\n",
    "    X_sample_not['instrument_reden_beeindiging_historie_succesvol'] = 0.0\n",
    "    y_proba_not = testing_session.run(None, {'X': X_sample_not.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Make predictions for a client with a successful instrumentation history\n",
    "    X_sample_successful = X_sample.copy()\n",
    "    X_sample_successful['instrument_reden_beeindiging_historie_succesvol'] = 1.0\n",
    "    y_proba_successful = testing_session.run(None, {'X': X_sample_successful.values.astype(np.float32)})[1][0]\n",
    "\n",
    "    # Append the likelihoods for both age groups\n",
    "    likelihoods_not.append([y_proba_not[1]])  # Probability of class 1 (fraud) for a client without a successful instrumentation history\n",
    "    likelihoods_successful.append([y_proba_successful[1]])  # Probability of class 1 (fraud) for a client with a successful instrumentation history\n",
    "\n",
    "# Convert likelihoods lists to NumPy arrays\n",
    "likelihoods_not = np.array(likelihoods_not)\n",
    "likelihoods_successful = np.array(likelihoods_successful)\n",
    "\n",
    "# Calculate the mean likelihoods for each group\n",
    "mean_likelihood_not = np.mean(likelihoods_not)\n",
    "mean_likelihood_successful = np.mean(likelihoods_successful)\n",
    "\n",
    "print(\"Mean likelihood for a client without a successful instrumentation history:\", mean_likelihood_not)\n",
    "print(\"Mean likelihood for a client with a successful instrumentation history:\", mean_likelihood_successful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba42895",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Combinatorial/fairness testing: \n",
    "# Evaluate the model's performance and predictions separately for different demographic groups (e.g., age groups, genders) and compare the outcomes to detect any disparities or biases.\n",
    "# Test case sets that test age groups, gender, if they have children and if they speak another language than dutch\n",
    "\n",
    "age_groups = {\n",
    "    'young_adult': [18, 30],  # 20-64 years old\n",
    "    'youngish_adult': [31, 40],  # 20-64 years old\n",
    "    'middle_aged_adult': [41, 50],  # 20-64 years old\n",
    "    'older_adult': [51, 60],  # 20-64 years old\n",
    "    'senior': [61, 120]  # 65+ years old (assuming 120 as upper limit)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "X_test_age = X_test.copy()\n",
    "\n",
    "# Extract age information from the test set\n",
    "X_test_age['age_group'] = pd.cut(X_test_age['persoon_leeftijd_bij_onderzoek'], bins=[0, 30, 40, 50, 60, 120], labels=['young_adult', 'youngish_adult', 'middle_aged_adult', 'older_adult', 'senior'])\n",
    "\n",
    "# Evaluate model performance for each age group\n",
    "for group in X_test_age['age_group'].unique():\n",
    "    # Filter test set for the current age group\n",
    "    X_group = X_test_age[X_test_age['age_group'] == group].drop(columns=['age_group'])\n",
    "    y_group = y_test[X_test_age['age_group'] == group]\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred_group = testing_session.run(None, {'X': X_group.values.astype(np.float32)})[0]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_group = accuracy_score(y_group, y_pred_group)\n",
    "    precision_group = precision_score(y_group, y_pred_group)\n",
    "    recall_group = recall_score(y_group, y_pred_group)\n",
    "    f1_score_group = f1_score(y_group, y_pred_group)\n",
    "\n",
    "    # Store results for the current age group\n",
    "    results[group] = {\n",
    "        'Accuracy': accuracy_group,\n",
    "        'Precision': precision_group,\n",
    "        'Recall': recall_group,\n",
    "        'F1 Score': f1_score_group\n",
    "    }\n",
    "\n",
    "# Print results for each age group\n",
    "for group, metrics in results.items():\n",
    "    print(f\"Results for {group} age group:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a0eca",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Gender test\n",
    "\n",
    "# Initialize a dictionary to store results for each gender group\n",
    "results = {}\n",
    "X_test_gender = X_test.copy()\n",
    "\n",
    "# Extract gender information from the test set\n",
    "X_test_gender['gender'] = X_test_gender['persoon_geslacht_vrouw'].apply(lambda x: 'woman' if x == 1.0 else 'man')\n",
    "\n",
    "# Evaluate model performance for each gender group\n",
    "for group in X_test_gender['gender'].unique():\n",
    "    # Filter test set for the current gender group\n",
    "    X_group = X_test_gender[X_test_gender['gender'] == group].drop(columns=['gender'])\n",
    "    y_group = y_test[X_test_gender['gender'] == group]\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred_group = testing_session.run(None, {'X': X_group.values.astype(np.float32)})[0]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_group = accuracy_score(y_group, y_pred_group)\n",
    "    precision_group = precision_score(y_group, y_pred_group)\n",
    "    recall_group = recall_score(y_group, y_pred_group)\n",
    "    f1_score_group = f1_score(y_group, y_pred_group)\n",
    "\n",
    "    # Store results for the current gender group\n",
    "    results[group] = {\n",
    "        'Accuracy': accuracy_group,\n",
    "        'Precision': precision_group,\n",
    "        'Recall': recall_group,\n",
    "        'F1 Score': f1_score_group\n",
    "    }\n",
    "\n",
    "# Print results for each gender group\n",
    "for group, metrics in results.items():\n",
    "    print(f\"Results for {group} gender group:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820abafc",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Language test\n",
    "\n",
    "# Initialize a dictionary to store results for each language group\n",
    "results = {}\n",
    "X_test_language = X_test.copy()\n",
    "\n",
    "# Extract language information from the test set\n",
    "X_test_language['language'] = X_test_language['persoonlijke_eigenschappen_spreektaal_anders'].apply(lambda x: 'other_language' if x == 1.0 else 'not')\n",
    "\n",
    "# Evaluate model performance for each language group\n",
    "for group in X_test_language['language'].unique():\n",
    "    # Filter test set for the current language group\n",
    "    X_group = X_test_language[X_test_language['language'] == group].drop(columns=['language'])\n",
    "    y_group = y_test[X_test_language['language'] == group]\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred_group = testing_session.run(None, {'X': X_group.values.astype(np.float32)})[0]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_group = accuracy_score(y_group, y_pred_group)\n",
    "    precision_group = precision_score(y_group, y_pred_group)\n",
    "    recall_group = recall_score(y_group, y_pred_group)\n",
    "    f1_score_group = f1_score(y_group, y_pred_group)\n",
    "\n",
    "    # Store results for the current language group\n",
    "    results[group] = {\n",
    "        'Accuracy': accuracy_group,\n",
    "        'Precision': precision_group,\n",
    "        'Recall': recall_group,\n",
    "        'F1 Score': f1_score_group\n",
    "    }\n",
    "\n",
    "# Print results for each language group\n",
    "for group, metrics in results.items():\n",
    "    print(f\"Results for {group} language group:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0202b3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# children test\n",
    "\n",
    "# Initialize a dictionary to store results for each children group\n",
    "results = {}\n",
    "X_test_children = X_test.copy()\n",
    "\n",
    "# Extract gender information from the test set\n",
    "X_test_children['children'] = X_test_children['relatie_kind_heeft_kinderen'].apply(lambda x: 'has_child' if x == 1.0 else 'not')\n",
    "\n",
    "# Evaluate model performance for each children group\n",
    "for group in X_test_children['children'].unique():\n",
    "    # Filter test set for the current children group\n",
    "    X_group = X_test_children[X_test_children['children'] == group].drop(columns=['children'])\n",
    "    y_group = y_test[X_test_children['children'] == group]\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred_group = testing_session.run(None, {'X': X_group.values.astype(np.float32)})[0]\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy_group = accuracy_score(y_group, y_pred_group)\n",
    "    precision_group = precision_score(y_group, y_pred_group)\n",
    "    recall_group = recall_score(y_group, y_pred_group)\n",
    "    f1_score_group = f1_score(y_group, y_pred_group)\n",
    "\n",
    "    # Store results for the current children group\n",
    "    results[group] = {\n",
    "        'Accuracy': accuracy_group,\n",
    "        'Precision': precision_group,\n",
    "        'Recall': recall_group,\n",
    "        'F1 Score': f1_score_group\n",
    "    }\n",
    "\n",
    "# Print results for each children group\n",
    "for group, metrics in results.items():\n",
    "    print(f\"Results for {group} children group:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735462f8",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Age test\n",
    "# Initialize variables to store counts\n",
    "same_predictions_count = 0\n",
    "total_samples = len(X_test)\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for age 25\n",
    "    X_sample_25 = X_sample.copy()\n",
    "    X_sample_25['persoon_leeftijd_bij_onderzoek'] = 25\n",
    "    y_pred_25 = testing_session.run(None, {'X': X_sample_25.values.astype(np.float32)})\n",
    "\n",
    "    # Make predictions for age 65\n",
    "    X_sample_65 = X_sample.copy()\n",
    "    X_sample_65['persoon_leeftijd_bij_onderzoek'] = 65\n",
    "    y_pred_65 = testing_session.run(None, {'X': X_sample_65.values.astype(np.float32)})\n",
    "\n",
    "    # Check if predictions are the same\n",
    "    if y_pred_25 == y_pred_65:\n",
    "        same_predictions_count += 1\n",
    "\n",
    "# Calculate the fraction of cases where the predictions are the same\n",
    "fraction_same_predictions = same_predictions_count / total_samples\n",
    "\n",
    "print(\"Fraction of cases where predictions are the same for age 25 and 65:\", fraction_same_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0931b3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Gender test\n",
    "# Initialize variables to store counts\n",
    "same_predictions_count = 0\n",
    "total_samples = len(X_test)\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for men\n",
    "    X_sample_men = X_sample.copy()\n",
    "    X_sample_men['persoon_geslacht_vrouw'] = 0.0\n",
    "    y_pred_men = testing_session.run(None, {'X': X_sample_men.values.astype(np.float32)})\n",
    "\n",
    "    # Make predictions for women\n",
    "    X_sample_women = X_sample.copy()\n",
    "    X_sample_women['persoon_geslacht_vrouw'] = 1.0\n",
    "    y_pred_women = testing_session.run(None, {'X': X_sample_women.values.astype(np.float32)})\n",
    "\n",
    "    # Check if predictions are the same\n",
    "    if y_pred_men == y_pred_women:\n",
    "        same_predictions_count += 1\n",
    "\n",
    "# Calculate the fraction of cases where the predictions are the same\n",
    "fraction_same_predictions = same_predictions_count / total_samples\n",
    "\n",
    "print(\"Fraction of cases where predictions are the same for men and women:\", fraction_same_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e7e28",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Language test\n",
    "# Initialize variables to store counts\n",
    "same_predictions_count = 0\n",
    "total_samples = len(X_test)\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for not\n",
    "    X_sample_not = X_sample.copy()\n",
    "    X_sample_not['persoonlijke_eigenschappen_spreektaal_anders'] = 0.0\n",
    "    y_pred_not = testing_session.run(None, {'X': X_sample_not.values.astype(np.float32)})\n",
    "\n",
    "    # Make predictions for other\n",
    "    X_sample_other = X_sample.copy()\n",
    "    X_sample_other['persoonlijke_eigenschappen_spreektaal_anders'] = 1.0\n",
    "    y_pred_other = testing_session.run(None, {'X': X_sample_other.values.astype(np.float32)})\n",
    "\n",
    "    # Check if predictions are the same\n",
    "    if y_pred_not == y_pred_other:\n",
    "        same_predictions_count += 1\n",
    "\n",
    "# Calculate the fraction of cases where the predictions are the same\n",
    "fraction_same_predictions = same_predictions_count / total_samples\n",
    "\n",
    "print(\"Fraction of cases where predictions are the same for dutch speakers and non-dutch speakers:\", fraction_same_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f31c4",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Children test\n",
    "# Initialize variables to store counts\n",
    "same_predictions_count = 0\n",
    "total_samples = len(X_test)\n",
    "\n",
    "# Iterate through each sample in the test set\n",
    "for index, row in X_test.iterrows():\n",
    "    # Convert the row to a DataFrame to ensure it's a DataFrame object\n",
    "    X_sample = pd.DataFrame(row).transpose()\n",
    "\n",
    "    # Make predictions for not\n",
    "    X_sample_not = X_sample.copy()\n",
    "    X_sample_not['relatie_kind_heeft_kinderen'] = 0.0\n",
    "    y_pred_not = testing_session.run(None, {'X': X_sample_not.values.astype(np.float32)})\n",
    "\n",
    "    # Make predictions for other\n",
    "    X_sample_children = X_sample.copy()\n",
    "    X_sample_children['relatie_kind_heeft_kinderen'] = 1.0\n",
    "    y_pred_children = testing_session.run(None, {'X': X_sample_children.values.astype(np.float32)})\n",
    "\n",
    "    # Check if predictions are the same\n",
    "    if y_pred_not == y_pred_children:\n",
    "        same_predictions_count += 1\n",
    "\n",
    "# Calculate the fraction of cases where the predictions are the same\n",
    "fraction_same_predictions = same_predictions_count / total_samples\n",
    "\n",
    "print(\"Fraction of cases where predictions are the same for people with or without children:\", fraction_same_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
